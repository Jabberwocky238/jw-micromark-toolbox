<p><a href="https://zhuanlan.zhihu.com/p/658443665">知乎</a>
<a href="/LLM Generate.md">LLM Generate</a>
<a href="/Continuous Batching.md">Continuous Batching</a>
<a href="/ORCA.md">ORCA</a>
当前，大模型推理sota的优化手段，主要是continuous batching和paged attention，</p>
<p>continuous batching能去除padding带来的冗余计算，提高系统吞吐；</p>
<p>paged attention能节省kv_cache padding带来的冗余显存，降低显存压力。</p>